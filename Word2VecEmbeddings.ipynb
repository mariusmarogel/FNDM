{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data and libraries"
      ],
      "metadata": {
        "id": "oGBIj_GFB1Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading csv datasets for Twitter15 and Twitter16 with links\n",
        "!gdown 1SaSq8kwvNmxq2HoQBenhXC3ejM8BU70d\n",
        "!gdown 1uGv2afj67P9BGEMwFPyv_IopjMzaqMuG"
      ],
      "metadata": {
        "id": "_JpznflIB5lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "LA9wdrabCFpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing for Word2Vec"
      ],
      "metadata": {
        "id": "0FGJAy8-CeZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Columns in downloaded datasets: d1 for Twitter15 and d2 for Twitter16\n",
        "tweet_id -> id of the source tweet\n",
        "text -> text content of the source tweet\n",
        "label -> label of the source tweet (True/False)\n",
        "n2v -> node2vec embedding of the source tweet with p=1,q=1 (not important for this part)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "r5E2uXfkDg9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = pd.read_csv('t15_text_n2v.csv', encoding='utf-8')\n",
        "d2 = pd.read_csv('t16_text_n2v.csv', encoding='utf-8')\n",
        "\n",
        "#Preprocess content for word2vec (list of lists).\n",
        "\n",
        "content1 = d1['text']\n",
        "content2 = d2['text']\n",
        "\n",
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "def remove_url(data):\n",
        "    url_tag=re.compile(r'URL')\n",
        "    data=url_tag.sub(r'', data)\n",
        "    return data\n",
        "\n",
        "def remove_double_spaces(data):\n",
        "    data = re.sub(' +', ' ', data)\n",
        "    return data\n",
        "\n",
        "def get_tokens(data):\n",
        "    return data.split(' ')\n",
        "\n",
        "def remove_stopwords(data):\n",
        "    data = ' '.join([word for word in data.split() if word not in stop_words])\n",
        "    return data"
      ],
      "metadata": {
        "id": "lG1FVVLUCZFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = pd.read_csv('t15_text_n2v.csv', encoding='utf-8')\n",
        "d2 = pd.read_csv('t16_text_n2v.csv', encoding='utf-8')\n",
        "\n",
        "content1 = d1['text']\n",
        "content2 = d2['text']"
      ],
      "metadata": {
        "id": "tD3bYixYDV-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content1=content1.apply(lambda z: remove_punctuations(z))\n",
        "content1=content1.apply(lambda z: remove_url(z))\n",
        "content1=content1.apply(lambda z: remove_double_spaces(z))\n",
        "content1=content1.apply(lambda z: remove_stopwords(z))\n",
        "content1=content1.apply(lambda z: get_tokens(z))\n",
        "\n",
        "content2=content2.apply(lambda z: remove_punctuations(z))\n",
        "content2=content2.apply(lambda z: remove_url(z))\n",
        "content2=content2.apply(lambda z: remove_double_spaces(z))\n",
        "content2=content2.apply(lambda z: remove_stopwords(z))\n",
        "content2=content2.apply(lambda z: get_tokens(z))"
      ],
      "metadata": {
        "id": "-3HmHxWKEQx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Word2Vec model from Gensim for word embeddings"
      ],
      "metadata": {
        "id": "DMmY5zJgGlzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default alpha=0.025, epochs=5, vector_size=100\n",
        "model1 = gensim.models.Word2Vec(window=10, min_count=4, sg=1, workers=1)\n",
        "model1.build_vocab(content1)\n",
        "model1.train(content1, total_examples=model1.corpus_count, epochs=model1.epochs)\n",
        "\n",
        "model2 = gensim.models.Word2Vec(window=10, min_count=4, sg=1, workers=1)\n",
        "model2.build_vocab(content2)\n",
        "model2.train(content2, total_examples=model2.corpus_count, epochs=model2.epochs)"
      ],
      "metadata": {
        "id": "Lk9qudPHGTUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab1_size = len(model1.wv.key_to_index) + 1\n",
        "vocab2_size = len(model2.wv.key_to_index) + 1\n",
        "(vocab1_size, vocab2_size)"
      ],
      "metadata": {
        "id": "c670SzS9Gelx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an Embedding Matrix for each dataset and save them"
      ],
      "metadata": {
        "id": "Q6exrQKlHtNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e1 = np.zeros((vocab1_size, 100))\n",
        "cnt = 1\n",
        "for word, i in model1.wv.key_to_index.items():\n",
        "  embedding_vector = model1.wv[word]\n",
        "  if embedding_vector is not None:\n",
        "    e1[cnt] = embedding_vector\n",
        "    cnt += 1\n",
        "\n",
        "e2 = np.zeros((vocab2_size, 100))\n",
        "cnt = 1\n",
        "for word, i in model2.wv.key_to_index.items():\n",
        "  embedding_vector = model2.wv[word]\n",
        "  if embedding_vector is not None:\n",
        "    e2[cnt] = embedding_vector\n",
        "    cnt += 1"
      ],
      "metadata": {
        "id": "WydXO2s4GfA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('t15_w2v_emb_matrix.npy', e1)\n",
        "np.save('t16_w2v_emb_matrix.npy', e2)"
      ],
      "metadata": {
        "id": "HRU0Ja8YH3oF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}