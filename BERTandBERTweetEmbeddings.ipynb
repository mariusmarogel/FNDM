{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F0B3POb4WFPR",
        "8w45quNkWuod",
        "OMQm8yvBXVpC",
        "EOiooBjyYBmA",
        "0evRq4VmYX6j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Datasets and libraries"
      ],
      "metadata": {
        "id": "F0B3POb4WFPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1SaSq8kwvNmxq2HoQBenhXC3ejM8BU70d\n",
        "!gdown 1uGv2afj67P9BGEMwFPyv_IopjMzaqMuG\n",
        "!pip install transformers\n",
        "!pip3 install emoji==0.6.0\n"
      ],
      "metadata": {
        "id": "gboCcY23WH1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import TFAutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "YakWMVwsWR1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing - For BERT and BERTweet removing punctuations, url and double spaces is enough"
      ],
      "metadata": {
        "id": "8w45quNkWuod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(data):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    data=punct_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "def remove_url(data):\n",
        "    url_tag=re.compile(r'URL')\n",
        "    data=url_tag.sub(r'', data)\n",
        "    return data\n",
        "\n",
        "def remove_double_spaces(data):\n",
        "    data = re.sub(' +', ' ', data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "v3reKk0lWeE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = pd.read_csv('t15_text_n2v.csv', encoding='utf-8')\n",
        "d2 = pd.read_csv('t16_text_n2v.csv', encoding='utf-8')\n",
        "\n",
        "d1['text']=d1['text'].apply(lambda z: remove_punctuations(z))\n",
        "d1['text']=d1['text'].apply(lambda z: remove_url(z))\n",
        "d1['text']=d1['text'].apply(lambda z: remove_double_spaces(z))\n",
        "\n",
        "d2['text']=d2['text'].apply(lambda z: remove_punctuations(z))\n",
        "d2['text']=d2['text'].apply(lambda z: remove_url(z))\n",
        "d2['text']=d2['text'].apply(lambda z: remove_double_spaces(z))\n",
        "\n",
        "d1.replace({False: 0, True: 1}, inplace=True)\n",
        "d2.replace({False: 0, True: 1}, inplace=True)"
      ],
      "metadata": {
        "id": "NytzfXbKW_KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Embeddings for Twitter15 and Twitter16"
      ],
      "metadata": {
        "id": "OMQm8yvBXVpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download pre-trained tokenizer and model from huggingface\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "6xgaAUd1Xcp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = d1['text']\n",
        "encoded_texts = tokenizer.batch_encode_plus(\n",
        "    texts,\n",
        "    add_special_tokens=True,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "input_ids = encoded_texts['input_ids']\n",
        "attention_masks = encoded_texts['attention_mask']\n",
        "t15_bert_embeddings = bert_model(input_ids, attention_mask=attention_masks)[0]"
      ],
      "metadata": {
        "id": "-iRA4fkvXNmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = d2['text']\n",
        "encoded_texts = tokenizer.batch_encode_plus(\n",
        "    texts,\n",
        "    add_special_tokens=True,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "input_ids = encoded_texts['input_ids']\n",
        "attention_masks = encoded_texts['attention_mask']\n",
        "t16_bert_embeddings = bert_model(input_ids, attention_mask=attention_masks)[0]"
      ],
      "metadata": {
        "id": "rcpSXvHwXqMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTweet Embeddings for Twitter15 and Twitter16"
      ],
      "metadata": {
        "id": "EOiooBjyYBmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
        "tweet_bert_model = TFAutoModel.from_pretrained('vinai/bertweet-base')"
      ],
      "metadata": {
        "id": "rdDCHHxpYD9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = d1['text']\n",
        "encoded_texts = tweet_tokenizer.batch_encode_plus(\n",
        "    texts,\n",
        "    add_special_tokens=True,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "input_ids = encoded_texts['input_ids']\n",
        "attention_masks = encoded_texts['attention_mask']\n",
        "t15_bertweet_embeddings = tweet_bert_model(input_ids, attention_mask=attention_masks)[0]"
      ],
      "metadata": {
        "id": "dx941NhGYKYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = d2['text']\n",
        "encoded_texts = tweet_tokenizer.batch_encode_plus(\n",
        "    texts,\n",
        "    add_special_tokens=True,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "input_ids = encoded_texts['input_ids']\n",
        "attention_masks = encoded_texts['attention_mask']\n",
        "t16_bertweet_embeddings = tweet_bert_model(input_ids, attention_mask=attention_masks)[0]"
      ],
      "metadata": {
        "id": "7YAyXZAjYQ14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Embeddings"
      ],
      "metadata": {
        "id": "0evRq4VmYX6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('t15_bert_emb.npy', t15_bert_embeddings)\n",
        "np.save('t16_bert_emb.npy', t16_bert_embeddings)\n",
        "np.save('t15_bertweet_emb.npy', t15_bertweet_embeddings)\n",
        "np.save('t16_bertweet_emb.npy', t16_bertweet_embeddings)"
      ],
      "metadata": {
        "id": "4WoPifhfYZvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}