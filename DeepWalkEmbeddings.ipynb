{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Datasets and libraries\n"
      ],
      "metadata": {
        "id": "qgMA88XfT4L3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FEVIQc5R0Tb"
      },
      "outputs": [],
      "source": [
        "# Downloading csv datasets for Twitter15 and Twitter16 with links\n",
        "!gdown 1SaSq8kwvNmxq2HoQBenhXC3ejM8BU70d\n",
        "!gdown 1uGv2afj67P9BGEMwFPyv_IopjMzaqMuG\n",
        "!gdown 1jfWwc8g-rS0G3oS5oKsydq8QXU7vev72\n",
        "!gdown 1z0vGTX5LGaMn-zjSpT9uIePXwi0qDBUz\n",
        "!mkdir Twitter15\n",
        "!mkdir Twitter16\n",
        "!unzip tree15.zip -d 'Twitter15'\n",
        "!unzip tree16.zip -d 'Twitter16'\n",
        "!pip install karateclub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from karateclub import DeepWalk\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZQOM8zybR8eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = pd.read_csv('t15_text_n2v.csv', encoding='utf-8')\n",
        "d2 = pd.read_csv('t16_text_n2v.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "xyC1SWdQSBnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions for reading a graph from a tree_file, and for drawing\n",
        "\n",
        "def read_graph_from_file(tree_dir, filename):\n",
        "    with open(os.path.join(tree_dir, filename), 'r') as file:\n",
        "        G = nx.DiGraph()\n",
        "        for line in file:\n",
        "            if '->' in line:\n",
        "                parent_node, child_node = line.strip().split('->')\n",
        "                G.add_edge(parent_node, child_node)\n",
        "    return G\n",
        "def draw_graph(G):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    pos = nx.kamada_kawai_layout(G)\n",
        "    node_options = {\"node_color\": \"red\", \"node_size\":30}\n",
        "    edge_options = {\"width\": .5, \"alpha\": .5, \"edge_color\":\"black\"}\n",
        "    nx.draw_networkx_nodes(G, pos, **node_options)\n",
        "    nx.draw_networkx_edges(G, pos, **edge_options)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Odq99wCCSE4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 32-dimensional DeepWalk Embeddings for Twitter15 and Twitter16"
      ],
      "metadata": {
        "id": "-CX-e0wvSbuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_dir = '/content/Twitter15/tree'"
      ],
      "metadata": {
        "id": "rnmGKAi4Uje2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t15_32d_emb_list = []\n",
        "for tweet_id in d1['tweet_id']:\n",
        "    d = {}\n",
        "    filename = str(tweet_id) + \".txt\"\n",
        "    G = read_graph_from_file(tree_dir, filename)\n",
        "    model = DeepWalk(walk_length=10, dimensions=32, window_size=5)\n",
        "    nodes = G.nodes()\n",
        "    nodes_list = list(nodes)\n",
        "    d = {nodes_list[i]: i for i in range(len(nodes_list))}\n",
        "    H = nx.relabel_nodes(G, d)\n",
        "    model.fit(H)\n",
        "    embs = model.get_embedding()\n",
        "    emb = embs[1]\n",
        "    t15_32d_emb_list.append(emb)"
      ],
      "metadata": {
        "id": "J0JPiSfFSFXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t16_32d_emb_list = []\n",
        "for tweet_id in d2['tweet_id']:\n",
        "    d = {}\n",
        "    filename = str(tweet_id) + \".txt\"\n",
        "    G = read_graph_from_file(tree_dir, filename)\n",
        "    model = DeepWalk(walk_length=10, dimensions=32, window_size=5)\n",
        "    nodes = G.nodes()\n",
        "    nodes_list = list(nodes)\n",
        "    d = {nodes_list[i]: i for i in range(len(nodes_list))}\n",
        "    H = nx.relabel_nodes(G, d)\n",
        "    model.fit(H)\n",
        "    embs = model.get_embedding()\n",
        "    emb = embs[1]\n",
        "    t16_32d_emb_list.append(emb)"
      ],
      "metadata": {
        "id": "SMr7yLF_SsAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#100-dimensional DeepWalk Embeddings on Twitter15 and Twitter16"
      ],
      "metadata": {
        "id": "IJHsoiE5TSKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_dir = '/content/Twitter16/tree'"
      ],
      "metadata": {
        "id": "8u8S_aKUUol9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t15_100d_emb_list = []\n",
        "for tweet_id in d1['tweet_id']:\n",
        "    d = {}\n",
        "    filename = str(tweet_id) + \".txt\"\n",
        "    G = read_graph_from_file(tree_dir, filename)\n",
        "    model = DeepWalk(walk_length=10, dimensions=100, window_size=5)\n",
        "    nodes = G.nodes()\n",
        "    nodes_list = list(nodes)\n",
        "    d = {nodes_list[i]: i for i in range(len(nodes_list))}\n",
        "    H = nx.relabel_nodes(G, d)\n",
        "    model.fit(H)\n",
        "    embs = model.get_embedding()\n",
        "    emb = embs[1]\n",
        "    t15_100d_emb_list.append(emb)"
      ],
      "metadata": {
        "id": "iGIEA2qvTXgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t16_100d_emb_list = []\n",
        "for tweet_id in d2['tweet_id']:\n",
        "    d = {}\n",
        "    filename = str(tweet_id) + \".txt\"\n",
        "    G = read_graph_from_file(tree_dir, filename)\n",
        "    model = DeepWalk(walk_length=10, dimensions=100, window_size=5)\n",
        "    nodes = G.nodes()\n",
        "    nodes_list = list(nodes)\n",
        "    d = {nodes_list[i]: i for i in range(len(nodes_list))}\n",
        "    H = nx.relabel_nodes(G, d)\n",
        "    model.fit(H)\n",
        "    embs = model.get_embedding()\n",
        "    emb = embs[1]\n",
        "    t16_100d_emb_list.append(emb)"
      ],
      "metadata": {
        "id": "We2pJiJKTbl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Embeddings"
      ],
      "metadata": {
        "id": "phOicvmhUDqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('32d/t15_dw_emb.npy', t15_32d_emb_list)\n",
        "np.save('32d/t16_dw_emb.npy', t16_32d_emb_list)\n",
        "np.save('100d/t15_dw_emb.npy', t15_100d_emb_list)\n",
        "np.save('100d/t15_dw_emb.npy', t15_100d_emb_list)"
      ],
      "metadata": {
        "id": "GIsEmnUwUFYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}